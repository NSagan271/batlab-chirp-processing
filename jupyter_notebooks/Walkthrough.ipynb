{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f395dc-257f-4784-a82e-31a0edf212c4",
   "metadata": {},
   "source": [
    "# Chirp Sequence Isolation and Chirp Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1311b0c4-ef00-4793-b2ef-4fe49742b73d",
   "metadata": {},
   "source": [
    "## Step 0: Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf47c81-b80d-405b-8ce8-1d540e0033d4",
   "metadata": {},
   "source": [
    "### 0.1: Brief Jupyter Lab Tutorial\n",
    "\n",
    "#### Notebook Kernel\n",
    "The **kernel** of a Jupyter notebook specifies the language in which the notebook is run.\n",
    "\n",
    "You need to use a Julia kernel for this notebook. If you haven't installed `IJulia`, please do so ([installation instructions](https://julialang.github.io/IJulia.jl/stable/manual/installation/)).\n",
    "If `IJulia` is installed properly, you should have the option to choose a Julia kernel (e.g. \"Julia 1.9.3\"), or Jupyter may select it automatically. You can see which kernel is selected by looking at the upper-right corner of this page. You can change the kernel by clicking on the kernel name.\n",
    "\n",
    "#### \"Help\" Tab\n",
    "The \"Help\" tab on the top bar has a lot of good resources. Here are some key ones for JupyterLab:\n",
    "\n",
    "- _\"Show Keyboard Shortcuts\"_: a list of all keyboard shortcuts\n",
    "- _\"Show Contextual Help\"_: opens a panel to the side that will show documentation for functions when you click on the function name in a code cell.\n",
    "- _\"Jupyter Reference\"_ and _\"JupyterLab Reference\"_: detailed documentation for using Jupyter notebooks. You probably don't need to look at this, but it might be a good read.\n",
    "\n",
    "#### Keyboard Shortcuts\n",
    "You can go to \"Help\" > \"Show Keyboard Shortcuts\" to see a list of keyboard shortcuts. Here's an abbreviated list, as well as some instructions for mouse-based notebook navigation:\n",
    "\n",
    "- Click on a code cell and press `SHIFT+ENTER` to run the cell. For the most part, the cells should be run in sequential order.\n",
    "  \n",
    "- Use `CTRL+ENTER` instead of `SHIFT+ENTER` if you don't want to move to the next cell after running the current one.\n",
    "  \n",
    "- When a code cell is being run, it will have `[*]` to the left of it. When execution is finished, the asterisk is replaced by a number, e.g. `[1]`. The number counts up for each cell you run (the first cell run will have `[1]`, the second cell run will have `[2]`, etc. If you run a cell multiple times, its number will keep on incrementing).\n",
    "  \n",
    "- Left click the white space to the left of a cell to select the cell without editing its code.\n",
    "  \n",
    "    - Doing this will reveal a blue vertical bar to the left of the cell. Clicking the bar will collapse the contents of the cell, and you can click the bar again to un-collapse the cell contents.\n",
    "      \n",
    "    - Clicking directly to the left of a cell output will put it into scroll mode. You can click the space to the left of the output again to make the output not scrollable.\n",
    "      \n",
    "- When you have a cell selected (but you're not editing it), you can press `a` to make a new cell above it and `b` to make a new cell below it.\n",
    "  \n",
    "- Press `SHIFT+L` (while not editing a cell) to show line numbers in the code cells.\n",
    "  \n",
    "- If the last line of a cell doesn't have a semicolon at the end, its value will be displayed below the cell. For instance, run the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f6325-2cf8-45b6-99c3-738b81cff337",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = π"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900499a-03f3-4ce3-a707-bc3902d88369",
   "metadata": {},
   "source": [
    "How do you get that pi symbol? Type `\\pi` in a code cell and press `TAB`. Then, wait for a little popup that says `π irrational {:π}` and either press `TAB` or `ENTER`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf7a9b3-b647-4e34-a785-2720a41d361a",
   "metadata": {},
   "source": [
    "### 0.2: Julia Resources\n",
    "\n",
    "If you want Julia documentation or a tutorial, look at [this page](https://julialang.org/learning/) of the official Julia website.\n",
    "\n",
    "**For documentation on the Batlab-specific code**, look at [**this page**](https://nsagan271.github.io/BatlabJuliaUtils/build/index.html).\n",
    "This also includes tips on common issues you may encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038302c7-5387-49b1-bca1-3852175e3be8",
   "metadata": {},
   "source": [
    "### 0.3: Have you set up the data?\n",
    "If not, here are some instructions:\n",
    "1. Go to the `matlab_utils` subdirectory of `batlab-chirp-processing` and use `tdms_to_mat.m` to convert some bat audio TDMS files to MAT files.\n",
    "   - It is recommended you start off with the `Pu166_01` and `Pu166_02` data from 10/05/2021, as this is what was used to test the notebook. Other experiments, like `Gr116`, will require you to try out different values of the parameters in the cells marked with `### CONFIGURABLE PARAMETERS`.\n",
    "   - To start off, you can save audio files to the `batlab-chirp-processing/data` directory. You can also use any other directory of your choosing.\n",
    "2. Copy all mic position files to `data`.\n",
    "4. Make a subdirectory of `data` called `centroid` and copy all relevant centroid data files to that directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bab57d-47ed-44ad-a0c8-af54e54bc158",
   "metadata": {},
   "source": [
    "### 0.4: Load the packages\n",
    "\n",
    "To install Julia packages:\n",
    "1. Go into the Julia command line. You can do this by running the `julia` command from any terminal, or by running the `julia` application.\n",
    "2. Hit the `]` key.\n",
    "3. Type `add Plots`, and then press Enter.\n",
    "4. Do the same for `Printf`, `MAT`, `Statistics`, `Roots`, `DataInterpolations`, `DSP`, and `WAV`.\n",
    "5. Press `Backspace` to exit package mode, and `CTRL+d` to exit the Julia command line.\n",
    "\n",
    "You will only have to install these packages once. Also, if you get an error like\n",
    "```\n",
    "ArgumentError: Package HelloWorld not found in current path,\n",
    "```\n",
    "you will have to repeat the above process, but for whichever package is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b5275a-ed88-48b0-86da-996f378255c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg;\n",
    "Pkg.develop(path=\"../BatlabJuliaUtils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f555e-5c97-450b-8548-2364894fad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using BatlabJuliaUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e03643-befb-4211-843f-0810e5cc0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some other packages\n",
    "## If this cell errors, please do the following:\n",
    "### 1. Go into the Julia command line.\n",
    "### 2. Press the \"]\" key.\n",
    "### 3. Run the command `add <Package>`, for all packages listed here. For\n",
    "###    example, `add Plots`, then `add Printf`, etc.\n",
    "using Plots;\n",
    "using Printf;\n",
    "using MAT;\n",
    "using Statistics;\n",
    "using Roots;\n",
    "using DataInterpolations;\n",
    "using DSP;\n",
    "using WAV;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee3c195-12a1-4ad4-aa58-91ceefa11d43",
   "metadata": {},
   "source": [
    "### 0.5: Define where the audio and centroid data is\n",
    "\n",
    "The following cells assume that you have added `Pu166_02` data to the `data` directory, as described in step **0.3**. To use different data, replace the filenames below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed628d2-c4f1-4021-8636-4ddc4e829233",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_FILENAME = \"../data/Pu166_02.mat\";\n",
    "CENTROID_FILENAME = \"../data/centroid/Pu166_002_centroidxyz.mat\";\n",
    "MIC_POSITION_FILENAME = \"../data/mic_positions_fall2021.mat\";\n",
    "\n",
    "CENTROID_VARIABLE_NAME = collect(keys(matread(CENTROID_FILENAME)))[1];\n",
    "MIC_POSITION_VARIABLE_NAME = collect(keys(matread(MIC_POSITION_FILENAME)))[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470cfcee-54a0-44b4-ab5e-270e141f7f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "(@printf \"Do the variable names for these MAT files look right?\\n\\tFor the centroid file: \\\"%s\\\",\\n\\tand for the mic position file: \\\"%s\\\"\" CENTROID_VARIABLE_NAME MIC_POSITION_VARIABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1fb5fe-0729-4ce3-9cd9-846eff1e677e",
   "metadata": {},
   "source": [
    "If the variable names for the MAT files don't look right, then uncomment and run the following two cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3024e8-3445-40ed-8581-7ccd08072fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# println(\"The variable names of the centroid file are:\\n\", keys(matread(CENTROID_FILENAME)), \"\\nand the MAT file looks like\")\n",
    "# centroids = matread(CENTROID_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61968c53-67f6-423f-86a0-7ee858206a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# println(\"The variable names of the mic position file are:\\n\", keys(matread(MIC_POSITION_FILENAME)), \"\\nand the MAT file looks like\")\n",
    "# mic_locations = matread(MIC_POSITION_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90acf67-541f-4968-9d43-743f8813b3e6",
   "metadata": {},
   "source": [
    "## Step 1: Read in the audio data\n",
    "\n",
    "The following cell reads the microphone data and creates a matrix `y`, where each column corresponds to a different microphone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc31815b-b875-4d9b-822d-57ce8aafaff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = readmicdata(AUDIO_FILENAME);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73848d36-953f-4829-8548-ea4f493a7b4c",
   "metadata": {},
   "source": [
    "**Plot the data**. The first argument is the range of indices (of the microphone data) to plot. For instance, `plotmicdata(1:100_000, y)` plots the first 100,000 samples of microphone data, for each microphone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17eb57e-06ca-490b-a958-39b01f286060",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotmicdata(1:100_000, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ce1e9-53d3-4b87-8423-57737d7e7c68",
   "metadata": {},
   "source": [
    "### Get a sample of noise for estimating the SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431392f7-a58a-4394-85b8-04785905d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_sample_idxs = getnoisesampleidxs(y);\n",
    "noise_sample = y[noise_sample_idxs, :];\n",
    "plotmicdata(noise_sample_idxs, y, title=\"Noise Sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c5774b-d017-441b-beee-58ea0e782d58",
   "metadata": {},
   "source": [
    "You can see that the noise is not zero-mean, so let's subtract out the mean of the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a3f4f-0813-4f80-b85a-86868ba0c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y .- mean(noise_sample; dims=1);\n",
    "noise_sample = noise_sample .- mean(noise_sample; dims=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161e41b-ad2d-4c40-ac4f-85cc0c9ad2b5",
   "metadata": {},
   "source": [
    "## Step 2: Determine which time segments have signal (for each mic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67323b0-9562-4840-aace-c0c77b65d167",
   "metadata": {},
   "source": [
    "### First, estimate the signal-to-noise ratio (SNR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b128f44-60cf-4d38-853b-bb7e3f4166bb",
   "metadata": {},
   "source": [
    "The next cell estimates the signal-to-noise ratio for every timepoint of the microphone data and saves it into a matrix `snr`, where `snr` has the same dimensions as `y`.\n",
    "The SNR is measured in decibels. In general, an SNR of 30 dB is considered decent signal strength, and higher SNR means that the signal is stronger.\n",
    "\n",
    "_The next cell might take a minute or two._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd834874-f253-4ea1-ab06-c1c1229d3fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = estimatesnr(y, noise_sample, window_size=128);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d66f9e-4444-43d7-ab86-4f5e997523f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotmicdata(1:100_000, snr, title=\"Signal-to-Noise Ratio\", ylabel=\"Decibels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbceacc-55c4-44b5-baee-d344982c0cb0",
   "metadata": {},
   "source": [
    "You can see that the peaks of the SNR correspond to the vocalizations in the microphone data we plotted earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498858f0-a92e-4397-9a14-902af85fb637",
   "metadata": {},
   "source": [
    "### What does the SNR look like during Buzz phase?\n",
    "\n",
    "When the bat is not in buzz phase, it is fairly easy to separate out chirps from segments that are just noise. However, the data is the messiest during buzz phase, so let's use the buzz phase to validate that chirp detection algorithms from future sections of this notebook work.\n",
    "\n",
    "**When do the buzz phase chirps happen?**\n",
    "\n",
    "You can either find buzz phase indices by playing around with the first argument  `plotmicdata`.\n",
    "Also, the following cell can give some good starting points (it makes crude estimates of chirp onsets, as heard by the microphones. Then it finds the times where chirp onsets are very close in time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b5fcb-aa5b-45f2-8cc3-9c63eb65b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatebuzzphase(snr, buzz_phase_chirp_separation_ms=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a1b84-401e-4cd7-8d3a-643119883187",
   "metadata": {},
   "source": [
    "Set the value of `INTERESTING_IDXS` below to the audio indices for the buzz phase, or any other indices of the microphone data you want to examine.\n",
    "\n",
    "Note that if the interval is too long, plotting will be slow and it will be hard to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065ed9f-2c72-4688-8515-da102ba122ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERESTING_IDXS = 1440297:1511542; # buzz phase for PU166_01\n",
    "INTERESTING_IDXS = 1313803:1405018; # buzz phase for PU166_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f19a9b2-d72f-4082-ace7-f6540c0dc143",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotmicdata(INTERESTING_IDXS, snr, title=\"Signal-to-Noise Ratio\", ylabel=\"Decibels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5997076-a068-4ec9-b4c2-8f607432eb71",
   "metadata": {},
   "source": [
    "### Separate Chirps from Noise\n",
    "The goal of this section is to find, approximately, where the vocalizations appear in each microphone's signal.\n",
    "We first find \"high-SNR regions\", which are regions where the SNR is above a certain threshold. We want each high-SNR region to contain _exactly one vocalization_ (and maybe some echos after the vocalization),\n",
    "\n",
    "This is done by setting thresholds on the SNR:\n",
    "- The SNR estimate is quite noisy, and we want to find contiguous regions that have signal. So, we apply what is known as a \"maximum filter\" to the SNR. For every timepoint, this filter returns the highest SNR in a `MAXFILTER_LENGTH` radius around that timepoint.\n",
    "- 30 db is considered \"good SNR\" for speech, so it makes sense to set the cutoff for what regions contain signal vs. noise around there.\n",
    "- In order to make sure that the high-SNR regions have good enough signal strength, we impose another constraint: at some point within the \"signal region\", the SNR must pass another, higher threshold. We describe how this threshold is determined in the comment at the end of the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28964d8-0b24-4b56-be9e-a379344b05d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIGURABLE PARAMETERS ###\n",
    "SIGNAL_THRESH = 30\n",
    "MAXFILTER_LENGTH_MS = 0.1 # If a point is at most MAXFILTER_LENGTH_MS milliseconds\n",
    "                          # from a point where the SNR is over SIGNAL_THRESH, then\n",
    "                          # that point is considered part of a signal region.\n",
    "MAXFILTER_LENGTH = Int64(round(MAXFILTER_LENGTH_MS / 1000 * FS));\n",
    "\n",
    "## The threshold imposed on the peak of a signal region has several components.\n",
    "## First, it must not be below MIN_PEAK_THRESH\n",
    "MIN_PEAK_THRESH = 35;\n",
    "## Second, it must be at least SNR_DROP_THRESH lower than the peak SNR in\n",
    "## PEAK_SNR_THRESH_RADIUS samples\n",
    "SNR_DROP_THRESH = 20;\n",
    "PEAK_SNR_THRESH_RADIUS = 2500;\n",
    "## The threshold is set as follows:\n",
    "## 1. We look around in a range of PEAK_SNR_THRESH_RADIUS samples around the \"signal region\"\n",
    "##    and we find the maximum SNR present in that range.\n",
    "## 2. We require that the peak SNR of the \"high-SNR region\" be at most\n",
    "##   SNR_DROP_THRESH decibels lower than that maximum value.\n",
    "## 3. We also require that the peak SNR be no lower than MIN_PEAK_THRESH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05501da0-9b1c-4302-91b8-76f68d9a911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_snr_locations = findhighsnrregions(snr, SIGNAL_THRESH, MIN_PEAK_THRESH, MAXFILTER_LENGTH,\n",
    "    snr_drop_thresh=SNR_DROP_THRESH, peak_snr_thresh_radius=PEAK_SNR_THRESH_RADIUS);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84850f7-2511-4456-805c-df2c11705a70",
   "metadata": {},
   "source": [
    "Let's see how well the algorithm did! If there is any critical section of the data (e.g., buzz phase vocalizations), you can make sure that the algorithm didn't mess up anywhere.\n",
    "\n",
    "Note that we will validate these sections later by only keeping chirp sequences where at least 2 microphones agree on the vocalization time (computed using the location data), so it's ok if there are some erroneous chirp segments, or the algorithm thinks some echos are chirp segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbba35d-a73e-4027-9190-593eb03e8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_idxs = INTERESTING_IDXS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269b3ba-e1a4-414c-9fe1-7bde56ca8cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = Array{Plots.Plot}(undef, 4);\n",
    "for mic=1:4\n",
    "    plots[mic] = plotmicdata(audio_idxs, y[:, mic], label=\"Mic Data\", title=(@sprintf \"Audio Data for Mic %d\" mic));\n",
    "    plot!(audioindextoms.(audio_idxs), high_snr_locations[audio_idxs, mic]*maximum(y[audio_idxs,mic]), label=\"High-SNR Regions\", color=:black, linewidth=1.2, legend=:bottomright);\n",
    "end\n",
    "plot(plots..., layout=grid(2, 2), size=(1500, 600), legend=:bottomright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df4967-9988-43c1-989e-3a0c85d4d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the start and end indices of every high-SNR region\n",
    "rough_high_snr_idxs_per_mic = Array{Matrix{Int64}}(undef, 4, 1);\n",
    "for mic=1:4\n",
    "    rough_high_snr_idxs_per_mic[mic] = findhighsnrregionidxs(snr, mic, SIGNAL_THRESH, MIN_PEAK_THRESH, MAXFILTER_LENGTH,\n",
    "        snr_drop_thresh=SNR_DROP_THRESH, peak_snr_thresh_radius=PEAK_SNR_THRESH_RADIUS);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5891566-da1e-4c81-88a5-74f033af7734",
   "metadata": {},
   "source": [
    "### Refine the ends of the high-SNR regions\n",
    "\n",
    "This method more or less gets the starts of the vocalizations, but it might cut off the end too early.\n",
    "\n",
    "So, we adjust the end of the chirp sequence using the `adjusthighsnridxs` function, which uses a similar algorithm to `findhighsnrregions`, except with a lower SNR threshold and longer max filter size. It also automatically prevents end of the chirp sequence from going past the beginning of the next chirp sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7c7240-2f51-4d03-a662-d6edd8d11abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIGURABLE PARAMETERS ###\n",
    "TAIL_SNR_THRESH=20;\n",
    "TAIL_MAXFILTER_LENGTH=50; # in samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97d454a-58ce-463d-a4c7-245f02095bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_snr_idxs_per_mic = Array{Matrix{Int64}}(undef, 4, 1);\n",
    "for mic=1:4\n",
    "    high_snr_idxs_per_mic[mic] = copy(rough_high_snr_idxs_per_mic[mic]);\n",
    "\n",
    "    N_seqs = size(high_snr_idxs_per_mic[mic], 1);\n",
    "    for row=1:N_seqs\n",
    "        max_end_idx = (row == N_seqs) ? size(snr, 1) : high_snr_idxs_per_mic[mic][row+1, 1];\n",
    "        high_snr_idxs_per_mic[mic][row, :] = \n",
    "                adjusthighsnridxs(snr, mic, rough_high_snr_idxs_per_mic[mic][row, :],\n",
    "                    max_end_idx, TAIL_SNR_THRESH, maxfilter_length=TAIL_MAXFILTER_LENGTH);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e2142-84b4-4f03-96ef-c632b7c18abb",
   "metadata": {},
   "source": [
    "Let's verify that the high-SNR regions produced are reasonable.\n",
    "\n",
    "The algorithm might pick up a few echos and think that they are chirp sequences. This is unavoidable, and some of these echos should be filtered out in the next section.\n",
    "It also might cut off the beginning of some chirp sequences if the beginning has very low SNR; we will compensate for this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ad3e9-09fe-4882-b7e5-5db33eb5e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mic = randint(4);\n",
    "while size(high_snr_idxs_per_mic[mic], 1) == 0\n",
    "    mic = randint(4);\n",
    "end\n",
    "seq_num = randint(size(high_snr_idxs_per_mic[mic], 1));\n",
    "(@printf \"Mic: %d, high-SNR region number: %d\\n\" mic seq_num);\n",
    "\n",
    "bounds = high_snr_idxs_per_mic[mic][seq_num, :];\n",
    "plotSTFTtime(y[bounds[1]:bounds[2], mic], noverlap=255, zero_pad=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ec458-d0e7-4dd6-9358-a8c3d1b25b8e",
   "metadata": {},
   "source": [
    "## Step 3: Determine chirp sequences that arose from the same initial bat vocalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc0280a-a60b-40c4-ad44-7dda54c59b4e",
   "metadata": {},
   "source": [
    "### Terminology: Chirp Sequences\n",
    "\n",
    "A chirp sequence is defined as all microphone outputs that result from a single bat vocalization.\n",
    "\n",
    "This is a somewhat overloaded term, which can mean one of two things:\n",
    "1. For a single microphone, a chirp and subsequent echos. We can call this a \"single-mic chirp sequence\".\n",
    "2. The chirp and subsequent echos, but for all microphones that picked up the chirp. We can call this a \"multi-mic chirp sequence\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a532cc1-dc77-4212-9655-341dbadd02b6",
   "metadata": {},
   "source": [
    "### Grouping Single-Mic Chirp Sequences to Make Multi-Mic Chirp Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b490124-016e-4bfb-9a61-390237e9c52c",
   "metadata": {},
   "source": [
    "Now that we know (approximately) where the chirp sequences are, for each microphone, we can group them by which ones came from the same initial chirp. Also, for robustness, we only keep chirp sequences where two or more microphones \"agree\" on the initial vocalization time, or where the SNR is very high.\n",
    "\n",
    "The vocalization time is calculated by interpolating the centroid data and solving for `t` in `distance_from_mic(t) = speed_of_sound * (time_chirp_reached_mic - t)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f0d743-2bdd-413d-84af-76802374a20c",
   "metadata": {},
   "source": [
    "### Read in centroid data and microphone locations\n",
    "\n",
    "We need the centroid data for this section, so let's read it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c9689d-f098-4284-a75c-660392f2b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = matread(CENTROID_FILENAME)[CENTROID_VARIABLE_NAME];\n",
    "if size(centroids, 1) == 3\n",
    "    centroids = Matrix(transpose(centroids));\n",
    "end\n",
    "centroids[end-9:end, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc589c8-777b-4600-bbcf-8f8a9f6e8f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_non_nan_idx = findfirst(.~isnan.(centroids[:, 1]));\n",
    "(@printf \"First time where the centroid data is not NaN: %d milliseconds\\n\" 1000*videoindextosec(first_non_nan_idx, size(centroids, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0d744e-a168-4a31-8cc9-bba223da8913",
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_positions = matread(MIC_POSITION_FILENAME)[MIC_POSITION_VARIABLE_NAME];\n",
    "if size(mic_positions, 1) == 3\n",
    "    mic_positions = Matrix(transpose(mic_positions));\n",
    "end\n",
    "mic_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1febe71e-bc6a-4c48-9509-34cf06a9e644",
   "metadata": {},
   "source": [
    "### Run `groupchirpsequencesbystarttime` to determine which chirp sequences came from the same vocalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a77da-3940-4841-9a51-fbed4036c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIGURABLE PARAMETERS ###\n",
    "TEMPORAL_TOLERANCE_MS = 2; #  if the estimated vocalization times for two\n",
    "                             # chirp sequences (for different microphones) are\n",
    "                             # within this number of milliseconds apart, they\n",
    "                             # are considered to be from the same vocalization.\n",
    "\n",
    "SINGLE_MIC_SNR_THRESH = 85;  # if a chirp sequence only has data from only one\n",
    "                             # microphone, still store the chirp sequence if it\n",
    "                             # has an SNR over this value.\n",
    "\n",
    "ANY_MIC_SNR_THRESH = 45; # One of the microphones needs to have an SNR over this\n",
    "                         # threshold, or else it's not a valid chirp sequence\n",
    "                         # (probably an echo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23997b64-c2d4-4b1f-b8cf-a816ff07ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "chirp_sequences, vocalization_times = \n",
    "        groupchirpsequencesbystarttime(high_snr_idxs_per_mic, snr, y, centroids,\n",
    "            mic_positions, single_mic_snr_thresh=SINGLE_MIC_SNR_THRESH,\n",
    "            any_mic_snr_thresh=ANY_MIC_SNR_THRESH,\n",
    "            vocalization_start_tolerance_ms=TEMPORAL_TOLERANCE_MS);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e1b6f2-a212-46b5-8acb-d3bcb9eb8df9",
   "metadata": {},
   "source": [
    "### Breaking down the return values of `groupchirpsequencesbystarttime`\n",
    "#### First Output: `chirp_sequences`\n",
    "\n",
    "We store the multi-mic chirp sequences as an `Array{Dict{Int, ChirpSequence}}`. This seems complicated, so let's break it down:\n",
    "- Each element of `chirp_sequences` corresponds to a different bat vocalization, i.e., each element of `chirp_sequences` is a multi-mic chirp sequence.\n",
    "  - Let's zoom in, for instance, on `chirp_sequences[1]`, aka the multi-mic chirp sequence corresponding to the first bat vocalization.\n",
    "  \n",
    "    An example of what a multi-mic chirp sequence looks like is `{1 => ChirpSequence(...), 3 => ChirpSequence(...), 4 => ChirpSequence(...)}`\n",
    "     \n",
    "     This is a mapping from a microphone number (in this case, the microphones are 1, 3, and 4) to a single-mic chirp sequence.\n",
    "\n",
    "    - Let's zoom in further to `chirp_sequences[1][4]`. This is the single-mic chirp sequence for what microphone 4 heard from the first bat vocalization.\n",
    "   \n",
    "      We represent this by a data struct: `ChirpSequence`. This contains the following fields:\n",
    "      1. `start_idx`: when the chirp reached the microphone, in number of samples since the beginning of the microphone data _originally_ read in.\n",
    "      2. `length`: number of audio samples in the single-mic chirp sequence.\n",
    "      3. `vocalization_time_ms`: approximate time of the bat vocalization, estimated using the onset of the chirp sequence in the microphone data and the distance of the bat to the respective microphone.\n",
    "      4. `snr_data`: SNR of the microphone data over the duration of the single-mic chirp sequence.\n",
    "      5. `mic_data`: oscilloscope data for this single-mic chirp sequence.\n",
    "      6. `mic_num`: microphone that the above data is from.\n",
    "     \n",
    "#### Second Output: `vocalization_times`\n",
    "\n",
    "This is the time, in milliseconds, of each bat vocalization. There is a one-to-one correspondence between indices of `vocalization_times` and indices of `chirp_sequences`.\n",
    "Each element of `vocalization_times` is the average of the vocalization times estimated for each microphone that heard the vocalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16c776b-5cff-4669-a00d-6ee1d9201f4b",
   "metadata": {},
   "source": [
    "### Visualizing `vocalization times`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bee453-b936-4d81-932f-7c4f17ff433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"There were \", length(vocalization_times), \" vocalizations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5520d77-66c6-439e-a9ff-e38d6a591c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot(vocalization_times, ones(length(vocalization_times)), line=:stem, marker=:circle, color=:1, markersize=5,\n",
    "    title=\"Vocalizations \", xlabel=\"Milliseconds\", ylabel=\"\", size=(1200, 200), yrange=(0, 1.2), legend=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921dfa70-b329-4c0e-be5c-d720d896d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot(vocalization_times[2:end]-vocalization_times[1:end-1], marker=:circle, ylabel=\"Milliseconds\", xlabel=\"Vocalization Number\",\n",
    "    title=\"Separation between subsequent vocalizations, in milliseconds (y-axis has a log scale)\", yaxis=:log, legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d98aae-26c0-4db0-8637-da96c85c28af",
   "metadata": {},
   "source": [
    "### Visualizing Chirp Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2287e747-63f7-4a59-94b4-52360ea64139",
   "metadata": {},
   "source": [
    "The following cell plots boxes around chirp sequences, labeled with their estimated vocalization time. Single-mic chirp sequences with the same colored box and same vocalization time belong to the same multi-mic chirp sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a07e6-247b-4988-9de3-019d72162d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotchirpsequenceboxes(audioindextoms(INTERESTING_IDXS[1]), audioindextoms(INTERESTING_IDXS[end]), vocalization_times, chirp_sequences, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d0e90d-4a6c-4701-aaa1-6833b16560eb",
   "metadata": {},
   "source": [
    "**Note**: If this doesn't look right and you suspect that there is an issue with the centroid data, take a look at the notebook `ValidateCentroidData.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d72b9-0c67-49e2-86b6-2e48de32b213",
   "metadata": {},
   "source": [
    "#### Let's plot some random chirp sequences!\n",
    "\n",
    "_Note_: zeros are added to the end of each single-mic chirp sequence so that they are all the same length. This is just for plotting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d1e16-b551-4c11-ab07-74594564a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_idx = randint(length(chirp_sequences));\n",
    "(@printf \"Sequence number: %d\\n\" seq_idx);\n",
    "plotchirpsequence(chirp_sequences[seq_idx], plot_spectrogram=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0107950-ee60-428f-a738-099e5b224cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can plot in the time domain too\n",
    "plotchirpsequence(chirp_sequences[seq_idx], plot_separate=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa49d9-40ab-4b48-a782-a07ec3382307",
   "metadata": {},
   "source": [
    "## Step 4: Estimate the \"melody\" of the vocalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34a25fe-5dbb-4b9b-873a-f1cab4d8ef76",
   "metadata": {},
   "source": [
    "Now that we have sufficiently pre-processed the data, we can work towards estimating the original vocalization made by the bat.\n",
    "We define the \"melody\" of the vocalization as the _fundmental harmonic_.\n",
    "\n",
    "The first step is \"tracing\" the melody:\n",
    "1. Find the time where the SNR is highest `FIND_HIGHEST_SNR_IN_FIRST_MS` milliseconds since the start of the chirp sequence, and find the melody of the chirp at that point (here, the SNR is hopefully high enough to accurately estimate the melody).\n",
    "2. Work backwards until the beginning of the chirp, at each index looking for the strongest frequency within some small range of the last frequency found.\n",
    "3. Repeat, but this time work towards the end of the chirp. To avoid picking up echos, enforce that, once the slope of the melody (with respect to time) becomes negative, it can never become positive.\n",
    "4. We have reached the end of the chirp when the tone drops off far enough from its maximum value.\n",
    "\n",
    "After tracing the melody, we need to see if we found the fundamental harmonic\n",
    "or some higher harmonic. This is done by taking the loudest part of the melody\n",
    "and dividing the frequency by 2, 3, etc. until we go below 20 kHz. The fundamental harmonic is the lowest such frequency with power at most\n",
    "`MELODY_DROP_THRESH_DB` below the loudest harmonic.\n",
    "\n",
    "Also, the function `smoothmelody` uses a moving average filter to produce a smoother version of the melody.\n",
    "\n",
    "**Note**: `findmelody` returns the fundamental harmonic in indices of the Fourier transform of length-`256` windows of the signal.\n",
    "\n",
    "`findmelodyhertz` will return the fundamental harmonic in hertz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef75b2e6-a74d-428b-b9ef-0a652970dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIGURABLE PARAMETERS ###\n",
    "MAXIMUM_MELODY_SLOPE = 5; # this is the maximum amount, in Fourier transform\n",
    "                          # indices, that the melody is allowed to change from\n",
    "                          # one index to the next.\n",
    "MELODY_DROP_THRESH_DB = 20; # described above\n",
    "FIND_HIGHEST_SNR_IN_FIRST_MS = 1.5; # described above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e45a790-ba4c-4299-81c4-cddd22936e32",
   "metadata": {},
   "source": [
    "The next cell estimates the melody of a random chirp and plots it on top of the spectrogram in blue. It also plots the estimated end index of the chirp in cyan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b6631-4d23-4785-9e52-a6e8b59fadae",
   "metadata": {},
   "outputs": [],
   "source": [
    "melody_kwargs = Dict{Symbol, Any}(\n",
    "    :maximum_melody_slope => MAXIMUM_MELODY_SLOPE,\n",
    "    :melody_drop_thresh_db => MELODY_DROP_THRESH_DB,\n",
    "    :find_highest_snr_in_first_ms => FIND_HIGHEST_SNR_IN_FIRST_MS,\n",
    "    :bandpass_filter => (20_000, 100_000)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7702a3-86a6-48c1-b9a9-1cbc36b63e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "findmelodyhertz(seq[mic_idx], MIN_PEAK_THRESH; melody_kwargs...);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92d262-58d5-4b11-b076-af009c899552",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_idx = randint(length(chirp_sequences));\n",
    "seq = chirp_sequences[seq_idx];\n",
    "mic_idx = collect(keys(seq))[randint(length(seq))];\n",
    "\n",
    "(@printf \"Sequence number: %d, mic: %d\\n\" seq_idx mic_idx);\n",
    "melody = findmelody(seq[mic_idx], MIN_PEAK_THRESH; melody_kwargs...);\n",
    "melody = smoothmelody(melody; filter_size=100);\n",
    "chirp_bounds = estimatechirpbounds(seq[mic_idx], melody, MIN_PEAK_THRESH);\n",
    "plotmelody(seq[mic_idx], melody, chirp_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f715ba-0661-4cfb-b9d8-f52a0ee84908",
   "metadata": {},
   "source": [
    "We find harmonics by searching for the strongest frequency in a range around a multiple of the melody. This is more accurate than just saying that the harmonic is exactly some multiple of the fundamental frequency, especially since the melody might get cut off at lower frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253747a2-89e0-4a3b-ae7a-f014dffb2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonic = getharmonic(seq[mic_idx], melody, 2);\n",
    "harmonic = smoothmelody(harmonic);\n",
    "plotmelody(seq[mic_idx], harmonic, chirp_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c7109b-316f-4728-bab6-79f85aa589c3",
   "metadata": {},
   "source": [
    "### Use the melody to approximately separate the chirp from the echos\n",
    "We can use the strength of the melody to approximately determine when the vocalization ends and the echos begin.\n",
    "\n",
    "The process is as follows:\n",
    "1. Find the point where the melody is the strongest.\n",
    "2. Find the first index, after this point, where the melody strength drops over\n",
    "    `MELODY_DROP_THRESH_DB` decibels from its peak value (if this cutoff value\n",
    "    is below `MELODY_THRESH_DB_LOW`, we instead find where the melody strength\n",
    "    goes below `MELODY_THRESH_DB_LOW`).\n",
    "    - If the melody strength never drops below this threshold, then just return\n",
    "        the last index of the chirp sequence.\n",
    "3. Apply a moving average filter to the melody strength.\n",
    "4. Apply the following heuristic:\n",
    "    - The end of the chirp is the first local minimum of the melody strength\n",
    "    after the index from step 2, or the first time the melody strength dips\n",
    "    below `MELODY_THRESH_DB_LOW`, whichever comes first.\n",
    "    - If neither event happens, return the last index of the chirp sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd53975-df2a-4ed4-a4dc-01d88606639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIGURABLE PARAMETERS ###\n",
    "# (parameters are described in the text above)\n",
    "MELODY_THRESH_DB_LOW = -20;\n",
    "MOVING_AVG_SIZE = 10;\n",
    "MELODY_DROP_THRESH_DB_START = 35; # the start index of the chirp is\n",
    "                                   # computed as the first index where\n",
    "                                   # the melody strength is greater than the\n",
    "                                   # maximum melody strength, minus this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb5fe0-2987-4fbb-8dbf-22a2528ef271",
   "metadata": {},
   "outputs": [],
   "source": [
    "chirp_bound_kwargs = Dict{Symbol, Any}(\n",
    "    :melody_drop_thresh_db => MELODY_DROP_THRESH_DB,\n",
    "    :melody_thresh_db_low => MELODY_THRESH_DB_LOW,\n",
    "    :melody_drop_thresh_db_start => MELODY_DROP_THRESH_DB_START,\n",
    "    :moving_avg_size => MOVING_AVG_SIZE\n",
    ");\n",
    "chirp_kwargs = merge(melody_kwargs, chirp_bound_kwargs);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83547929-c4ce-4a29-b075-d1e85008cdf9",
   "metadata": {},
   "source": [
    "The next cell plots the estimated chirps (for a random chirp sequence) on top of the full chirp sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d844e-1b3a-4aa7-955e-8eab11ae313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_idx = randint(length(chirp_sequences));\n",
    "(@printf \"Sequence number: %d\\n\" seq_idx);\n",
    "seq = chirp_sequences[seq_idx];\n",
    "plotestimatedchirps(seq, MIN_PEAK_THRESH; chirp_kwargs...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7c9e34-aa71-4bae-9671-ce7d5821e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotchirpsequence(chirp_sequences[seq_idx], plot_spectrogram=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c297bf-879b-4c5a-bb42-5c1d48b70570",
   "metadata": {},
   "source": [
    "### Aligning Chirps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0b33c-ea3d-4059-87e3-706e44aee568",
   "metadata": {},
   "source": [
    "You may notice that for Pu166/168 data, the beginnings of the chirps are often cut off due to low SNR. The parts that are cut off have such low SNR that we can't do anything with them, but it will matter in Step 5 that chirps from all microphones can be temporally aligned with each other.\n",
    "\n",
    "So, we zero-pad the beginnings of those sequences such that that they are aligned chirp sequences from the other mics.\n",
    "\n",
    "You can run the next few cells to see this in action, and look at the documentation for `computemelodyoffsets` for more information. The output of `computemelodyoffsets` is the number of samples cut off from the beginning of each microphone's chirp sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18629a3b-7b02-4331-ac52-f97813805992",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_idx = randint(length(chirp_sequences));\n",
    "(@printf \"Sequence number: %d\\n\" seq_idx);\n",
    "offsets = computemelodyoffsets(chirp_sequences[seq_idx], MIN_PEAK_THRESH; chirp_kwargs...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b78a43-8b1e-4766-a051-53e304dfee26",
   "metadata": {},
   "source": [
    "The following cell plots the chirp sequence for each microphone, zero-padded at the beginning so that the chirps are aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8dc941-4a28-4f44-8735-4d2eec10a2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotoffsetchirps(chirp_sequences[seq_idx], offsets, getchirpstartandendindices(chirp_sequences[seq_idx], MIN_PEAK_THRESH; chirp_kwargs...)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60e09cb-22cf-449e-8144-857ed99983a2",
   "metadata": {},
   "source": [
    "## Step 5: Use optimization to combine information from all microphones and estimate the actual bat vocalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5037b58b-dc6c-4925-8210-ddc80fd1e8c1",
   "metadata": {},
   "source": [
    "We can combine the data from different microphones through **blind deconvolution**, which involves estimating:\n",
    "\n",
    "1. The actual bat vocalization, $\\mathbf{x}$, that produced a given chirp sequence\n",
    "2. A set of impulse responses $\\mathbf{h}_k$ such that, if $\\mathbf{y}_k$ is the data from microphone $k$ for the chirp sequence, the convolution relation $\\mathbf{x} * \\mathbf{h}_k \\approx \\mathbf{y}_k$ holds.\n",
    "\n",
    "We don't have enough information to solve the problem, unless we make some assumptions about the vocalization and the impulse response. We assume the following:\n",
    "\n",
    "1. The impulse responses should be somewhat sparse (most elements are close to zero). Intuatively, it should be nonzero at the beginning of the chirp sequence and at the locations of echos.\n",
    "2. The melody estimation from the previous section was relatively accurate. So, we can expect most of the energy of the spectrogram of the chirp to be at the melody and its harmonics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f16ebb-83b9-45b7-a500-d5fa66fb80e5",
   "metadata": {},
   "source": [
    "### Utility of this section\n",
    "\n",
    "The previous sections do pretty well at estimating vocalizations, and the optimization algorithm doesn't do too much better. It can typically remove echos from estimated chirps and clean up some spectral noise, but it does occasionally introduce spectral noise.\n",
    "\n",
    "This section can be seen as an interesting theoretical extension: formulating echo denoising as a blind deconvolution problem and jointly computing the initial vocalization and the impulse responmse mapping the vocalization to each microphone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc4e9a0-b713-44af-bbf5-55d4c22a93bf",
   "metadata": {},
   "source": [
    "### Optimization problem formulation\n",
    "\n",
    "Overall, we want to optimize the following things:\n",
    "\n",
    "1. **Data fitting**: how close $\\mathbf{x} * \\mathbf{h}_k$ is to the microphone data, $\\mathbf{y}_k$, for the given chirp sequence.\n",
    "2. **Impulse response sparsity**: how many nonzeros $\\mathbf{h}_k$ has, approximated (for the sake of the optimization algorithm) by the absolute sum of its elements.\n",
    "3. **Proximity to estimated melody**: if we don't try to enforce any constraints on what the chirp, $\\mathbf{x}$, looks like, the algorithm provides a pretty noisy chirp. So, we constrain that $\\mathbf{x}$ should loosely follow the melody we estimated in step 4.\n",
    "   - *This appears the least reliable part of the optimization objective, mainly because chirps can either be \"narrow\" or diffuse in frequency.*\n",
    "\n",
    "You will be able to choose how much each of these three objectives \"matter\" before performing optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e2655-a20e-4553-8f07-d19ee3e8e985",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "Let's grab a chirp sequence to perform this optimization on!\n",
    "We will construct the matrix $\\mathbf{Y} \\triangleq \\begin{bmatrix} \\mathbf{y}_{k_1} & \\mathbf{Y}_{k_2} & \\cdots\\end{bmatrix}$ (where $k_1$, $k_2$, etc. are the microphone indices used for this chirp sequence).\n",
    "\n",
    "_Note_: f the chirp sequences have wildly different amplitudes, the optimization algorithm tends to ignore those with lower amplitudes. So, we normalize each chirp sequence to have an amplitude of 1 when constructing the matrix $\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64bac8e-84d7-4f36-9bff-a9a14de9fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_idx = randint(length(chirp_sequences));\n",
    "chirp_seq = chirp_sequences[seq_idx];\n",
    "(@printf \"Sequence number: %d\\n\" seq_idx);\n",
    "\n",
    "pad_len = 300; \n",
    "@assert pad_len > 128\n",
    "offsets = computemelodyoffsets(chirp_seq, MIN_PEAK_THRESH; chirp_kwargs...)\n",
    "Y, mics = getchirpsequenceY(chirp_seq, offsets, pad_len);\n",
    "plotchirpsequence(chirp_seq; plot_spectrogram=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97456b48-891f-45ef-b449-519439a154e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotestimatedchirps(chirp_seq, MIN_PEAK_THRESH; chirp_kwargs...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921e0e40-3c9b-45ae-84f6-885ac56ce697",
   "metadata": {},
   "source": [
    "### Initial condition\n",
    "\n",
    "We can give the optimization algorithm a \"warm start\" using the estimated chirps from Step 4.\n",
    "To do so, we estimate the chirp and impulse response for each microphone, and choose the microphone that produces the sparsest impulse response.\n",
    "\n",
    "There are two different methods for finding the initial condition. The first, and **recommended**, is to use `getinitialconditionsnr`, which chooses the microphone with the highest SNR to provide the initial guess for $\\mathbf{x}$. The second, using `getinitialconditionsparsity`, is to choose whichever microphone provides the sparsest estimates of the impulse responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6458dd2-436b-4d3b-b987-6d35e9df2fe1",
   "metadata": {},
   "source": [
    "There is one parameter: `H_FFT_THRESH`: frequency components of the impulse response initial condition at indices where the FFT of `X_init` is less than this threshold will be set to zero. This helps the optimization algorithm converge to a sparse impulse response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92fbe9b-926a-4edf-aa7e-bb2ab20acf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIGURABLE PARAMETER ###\n",
    "H_FFT_THRESH = 0.1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e1e288-8584-4155-b9d7-890779b78b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_init, H_init, longest_chirp = getinitialconditionsnr(Y, chirp_seq, mics, MIN_PEAK_THRESH, h_fft_thresh=H_FFT_THRESH; chirp_kwargs...)\n",
    "plotSTFTtime(X_init; title=\"Chirp initial condition\", nfft=256, noverlap=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9651aa08-ffd3-483c-b772-55a43f7c319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotmicdata(H_init, title=\"Impulse response initial condition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d919c132-45aa-44e5-9640-44f91d9a408e",
   "metadata": {},
   "source": [
    "### Optimization parameters\n",
    "You can choose how much to weight each term in the optimization problem: think of these parameters as percentages of how important each component of the optimization problem is, though they need not sum up to 100. The following cell has some reasonable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5d687-d22f-4c4b-a032-ede8a1890f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIGURABLE PARAMETERS ###\n",
    "\n",
    "DATA_FITTING_WEIGHT = 70; ## Making H * X close to Y\n",
    "H_SPARSITY_WEIGHT = 5; ## Looking for sparse impulse responses. Generally, should be weighted pretty low.\n",
    "MELODY_WEIGHT = 35; ## Maing melody of the chirp close to the result of findmelody"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaafdff-2bed-419a-9d95-5b3b4ca382f6",
   "metadata": {},
   "source": [
    "How spaced out should the spectrogram windows be? A good heuristic is to have 50 windows over the length of the chirp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa1a9a-d697-42d4-982c-f314fcf5db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "STFT_STRIDE = Int64(ceil(longest_chirp / 50));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe1857-bed5-407a-9f87-b1e5dc67b49a",
   "metadata": {},
   "source": [
    "How long to run the optimization algorithm for? 10000 iterations seems to work pretty well; running it for longer may cause the algorithm to converge better if it doesn't converge well already (presumably, giving better results), and running for fewer iterations makes it faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aa577d-6c57-45fe-9ca8-f1382a63b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIGURABLE PARAMETER ###\n",
    "\n",
    "MAX_ITER=10000;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e85af2-0e2e-4b8f-af26-ccdf4c1a1376",
   "metadata": {},
   "source": [
    "How \"diffuse\" is the melody in frequency? This is determined by a pair of parameters called `MELODY_RADIUS_START` and `MELODY_RADIUS_END`, which are approximations of how diffuse the melody is (approximately, in kilohertz) at the start and end of the chirp (as chirps tend to become narrower at the end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff82a78-a422-4771-aecd-fcfb8c845868",
   "metadata": {},
   "outputs": [],
   "source": [
    "MELODY_RADIUS_START = 10; MELODY_RADIUS_END = 0; # NOT BUZZ\n",
    "# MELODY_RADIUS_START = 15; MELODY_RADIUS_END = 5; # BUZZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1bf077-caf9-4bf6-b927-e027747e6f77",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Run the following cell to see what the optimization algorithm thinks the bat vocalization was (along with the impulse responses of each microphone for the given chirp)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f6f29-92c8-450e-b087-81ab9073e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_opt, H_opt, max_chirp_len = optimizePALM(chirp_seq, mics, Y, H_init, X_init, MIN_PEAK_THRESH,\n",
    "                        DATA_FITTING_WEIGHT, H_SPARSITY_WEIGHT, MELODY_WEIGHT;\n",
    "                        melody_radius_start=MELODY_RADIUS_START, melody_radius_end=MELODY_RADIUS_END,\n",
    "                        max_iter=MAX_ITER, nfft=256, stft_stride=STFT_STRIDE, chirp_kwargs...);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711a2125-a4e8-4c99-9f5c-76d337ea14c6",
   "metadata": {},
   "source": [
    "_Note_: if it is having a hard time finding a sparse impulse response, try setting `H_SPARSITY_WEIGHT` to `50` and increase the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2687e771-42f7-4ed5-8dda-4556afc60755",
   "metadata": {},
   "source": [
    "### Optimization Results: Vocalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bac360-f3a7-455a-b7bf-bedcdeb5471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotmicdata(X_opt, label=\"x\", title=\"Optimization Output: Time Domain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c652b0-658d-47c1-8a09-3998d002792b",
   "metadata": {},
   "source": [
    "#### Play the estimated chirp, slowed down by a factor of 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12f4ab-6bfe-4c9b-b959-5a6b598ab519",
   "metadata": {},
   "outputs": [],
   "source": [
    "using WAV;\n",
    "SLOW_DOWN_FACTOR = 15;\n",
    "wavplay(X_opt[128:max_chirp_len] ./ maximum(abs.(X_opt)), round(FS / SLOW_DOWN_FACTOR));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd8356-ffda-4078-a472-2ecd2db00353",
   "metadata": {},
   "source": [
    "#### Frequency-Domain Results: optimization algorithm estimation of the vocalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6f50e6-a66f-4212-8076-d02258c9bd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSTFTtime(X_opt[128:max_chirp_len+128], title=\"Optimization Output: Vocalization\", noverlap=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919784d4-bce1-45c4-a1f8-029ac6ddbeed",
   "metadata": {},
   "source": [
    "#### For comparison: vocalization initial condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a693706-b651-4750-847c-35712d012efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSTFTtime(X_init[128:max_chirp_len+128], title=\"Initial Condition: Vocaliztion\", noverlap=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f132c-f6b7-4538-b098-a07c39fdb876",
   "metadata": {},
   "source": [
    "### Optimization Results: Impulse Responses\n",
    "\n",
    "**Note** sometimes there might be significant peaks at the *end* of the impulse responses. This is not unexpected! We are actually using the circular convolution of the vocalization and impulse response to estimate the microphone data, so those peaks at the end can be thought of as occuring before the pictured start of the impulse response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93793c1-79ec-42d5-a9ce-c201950b49e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=size(Y, 2);\n",
    "plots = Matrix(undef, 1, 5);\n",
    "for i=1:K\n",
    "    plots[i] = plotmicdata(H_opt[:, i], title=(@sprintf \"Impulse response for mic %d\" mics[i]), legend=false)\n",
    "end\n",
    "plots[K+1] = myplot([0, 0], legend=false, title=\"Blank Plot\", xlabel=\"\", ylabel=\"\");\n",
    "\n",
    "num_rows = Int64(floor(K + K%2) / 2);\n",
    "plot(plots[1:num_rows*2]..., layout=(num_rows, 2), size=(1100, num_rows*300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff25360-01df-4941-be83-21a8e084803b",
   "metadata": {},
   "source": [
    "### Optimization Results: Estimation Error\n",
    "\n",
    "How far is $\\mathbf{x} * \\mathbf{h}_k$ from the actual microphone data? For the right choise of optimization parameters, the estimation error (orange curve plotted below) should be insignificant (same order of magnitude as noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a9128-7cfa-4ea7-a873-e58278f53126",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=size(Y, 2);\n",
    "plots = Matrix(undef, 1, 5);\n",
    "Y_hat = circconv(X_opt, H_opt);\n",
    "for i=1:K\n",
    "    plots[i] = plotmicdata([Y[:, i] Y_hat[:, i]-Y[:, i]], title=(@sprintf \"Mic data and estimation error for mic %d\" mics[i]), label=[\"Mic data\" \"Error\"])\n",
    "end\n",
    "plots[K+1] = myplot([0, 0], legend=false, title=\"Blank Plot\", xlabel=\"\", ylabel=\"\");\n",
    "\n",
    "num_rows = Int64(floor(K + K%2) / 2);\n",
    "plot(plots[1:num_rows*2]..., layout=(num_rows, 2), size=(1100, num_rows*300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a3632-24e5-419f-966c-41581428e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=size(Y, 2);\n",
    "plots = Matrix(undef, 1, 5);\n",
    "for i=1:K\n",
    "    plots[i] = plotmicdata(Y_hat[:, i], title=\"Est. chirp convolved with est. impulse response\")\n",
    "end\n",
    "plots[K+1] = myplot([0, 0], legend=false, title=\"Blank Plot\", xlabel=\"\", ylabel=\"\");\n",
    "\n",
    "num_rows = Int64(floor(K + K%2) / 2);\n",
    "plot(plots[1:num_rows*2]..., layout=(num_rows, 2), size=(1100, num_rows*300))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
